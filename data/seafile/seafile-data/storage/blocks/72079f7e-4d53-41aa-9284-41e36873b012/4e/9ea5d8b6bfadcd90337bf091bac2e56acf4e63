## 1. helm 中主要修改的 values
```yaml
ingress:
  annotations: null
  enabled: true
  host: longhorn.testlab.net
  ingressClassName: nginx

service:
  manager:
    loadBalancerIP: ''
    loadBalancerSourceRanges: ''
    nodePort: ''
    type: ClusterIP
  ui:
    nodePort: null
    type: LoadBalancer
```

此时已经为 longhorn UI 新添加 ingress \
longhorn 副本 `defaultClassReplicaCount` 应该和可调度节点数量一致
## 2. metrics
Longhorn 后端服务是指向 Longhorn manager pods 的服务。Longhorn 天然在 [http://LONGHORN_MANAGER_IP:PORT/metrics](http://longhorn_manager_ip:PORT/metrics) 的 endpoint 上暴露了 metrics 指标。 
- ingreessroute 暴露到集群外即可
```
apiVersion: traefik.io/v1alpha1
kind: IngressRoute
metadata:
  name: longhorn-backend-metrics
  namespace: longhorn-system
spec:
  entryPoints:
    - web
  routes:
    - kind: Rule
      match: Host(`longhorn-backend.testlab.net`) && Path(`/metrics`)
      services:
        - name: longhorn-backend
          port: 9500
```

## 3. debug
```
metrics:
  serviceMonitor:
    enabled: false # 默认false ，修改为true开启之后，出现错误
```
>[!fail]
>这是同时需要部署一个 prometheus-operater，并通过集群内的标签进行识别\
> `# no matches for kind “ServiceMonitor“ in version “monitoring.coreos.com/v1`

>[!solution]
>kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml \
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml \
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml \
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml \
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml \
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml \
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml \
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml
## 4. Ingress debug
- 这个 yaml 文件的 ingress 由 longhorn 的 helm 修改后生成，可以正常代理转发
%% REGION %%
 ```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    meta.helm.sh/release-name: longhorn
    meta.helm.sh/release-namespace: longhorn-system
  creationTimestamp: '2024-03-14T10:41:32Z'
  generation: 1
  labels:
    app: longhorn-ingress
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/version: v1.5.4
    helm.sh/chart: longhorn-102.3.2_up1.5.4
  managedFields:
    - apiVersion: networking.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:meta.helm.sh/release-name: {}
            f:meta.helm.sh/release-namespace: {}
          f:labels:
            .: {}
            f:app: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/version: {}
            f:helm.sh/chart: {}
        f:spec:
          f:rules: {}
      manager: helm
      operation: Update
      time: '2024-03-14T10:41:32Z'
  name: longhorn-ingress
  namespace: longhorn-system
  resourceVersion: '141451'
  uid: 6beae80f-7167-49f8-a982-b9a9b925ce10
spec:
  rules:
    - host: longhorn.testlab.net
      http:
        paths:
          - backend:
              service:
                name: longhorn-frontend
                port:
                  number: 80
            path: /
            pathType: ImplementationSpecific
status:
  loadBalancer: {}
```
%% ENDREGION %%

- 这个文件是自行修改longhorn 的backend 为loadbalancer类型，并创建的ingress
- 相同的文件创建之后，起初正常，可以在 host:9500/metrics 访问
- 之后出现异常，在集群相同的网段可以正常得到数据，在非集群网段不能得到数据
- 最终，忽然生效并不再异常!!!前后对比 ingress 并无异常（重新部署，正常转发时发现 metallb 的 controller 的日志显示正常）

%% REGION %%
``` yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  creationTimestamp: '2024-03-15T01:52:54Z'
  generation: 3
  managedFields:
    - apiVersion: networking.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:spec:
          f:rules: {}
      manager: agent
      operation: Update
      time: '2024-03-15T03:04:03Z'
  name: longhorn-backend
  namespace: longhorn-system
  resourceVersion: '449501'
  uid: c7de0683-ee74-47de-9578-a9f06dcd0153
spec:
  rules:
    - host: longhorn-backend.testlab.net
      http:
        paths:
          - backend:
              service:
                name: longhorn-backend
                port:
                  number: 9500
            path: /
            pathType: Prefix
status:
  loadBalancer: {}
```
%% ENDREGION %%

排查参考--volume degraded status
[Longhorn 云原生容器分布式存储 - 故障排除指南-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/1893093)