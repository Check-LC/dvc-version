# 测试环境

部署方式：使用测试环境中的k8s集群，使用ECK helm chart部署三master节点的elasticsearch集群 和 kibana

* k8s集群：test-sys-k8s
* namespace：elasticseasrch
* helm chart： elastic https://helm.elastic.co
* Chart：

  
  1. eck-elasticsearch
  2. eck-kibana

官方 eck-elastic文档：


1. [https://www.elastic.co/guide/en/cloud-on-k8s/master/k8s-deploy-eck.html]()
2. [Install ECK using the Helm chart](https://www.elastic.co/guide/en/cloud-on-k8s/master/k8s-install-helm.html)


# 1.部署ECK-Elasticsearch + ECK-Kibana

1\.添加ECK helm仓库

```yaml
 helm repo add elastic https://helm.elastic.co
```


2\.使用helm安装elastic-operator（安装elastic-operator会自动安装elastic-operator-crds）

This chart is usually automatically installed by the [ECK Operator Helm Chart](https://artifacthub.io/packages/helm/elastic/eck-operator) when installed using the default settings.

```yaml
helm install elastic-operator elastic/eck-operator -n elastic-system --create-namespace
```


3\.使用helm安装ECK-kibana ECK-Elasticsearch

## kibana values.yaml

```yaml
---
# Default values for eck-kibana.
# This is a YAML-formatted file.

# Overridable names of the Kibana resource.
# By default, this is the Release name set for the chart,
# followed by 'eck-kibana'.
#
# nameOverride will override the name of the Chart with the name set here,
# so nameOverride: quickstart, would convert to '{{ Release.name }}-quickstart'
#
# nameOverride: "quickstart"
#
# fullnameOverride will override both the release name, and the chart name,
# and will name the Kibana resource exactly as specified.
#
# fullnameOverride: "quickstart"

# Version of Kibana.
#
version: 8.13.0

# Labels that will be applied to Kibana.
#
labels: {}

# Annotations that will be applied to Kibana.
#
annotations: {}

spec:
  # Count of Kibana replicas to create.
  #
  count: 1

  # Reference to ECK-managed Elasticsearch resource.
  #
  elasticsearchRef:
    name: test-eck-elasticsearch # 连接到es集群的集群名称，注意如果连接es集群失败，Kibana无法启动
    # Optional namespace reference to Elasticsearch resource.
    # If not specified, then the namespace of the Kibana resource
    # will be assumed.
    #
    # namespace: default 
  podTemplate:
    spec:
    containers:
    - name: kibane
      env:
      - name: NODE_OPTIONS
        value: "--max-old-space-size=2048"
      resources:
        requests:
          memory: 1Gi
          cpu: 0.5
        limits:
          memory: 2.5Gi
          cpu: 2
```

## elasticsearch values.yaml

```yaml
---
# Default values for eck-elasticsearch.
# This is a YAML-formatted file.

# Overridable names of the Elasticsearch resource.
# By default, this is the Release name set for the chart,
# followed by 'eck-elasticsearch'.
#
# nameOverride will override the name of the Chart with the name set here,
# so nameOverride: quickstart, would convert to '{{ Release.name }}-quickstart'
#
# nameOverride: "quickstart"
#
# fullnameOverride will override both the release name, and the chart name,
# and will name the Elasticsearch resource exactly as specified.
#
# fullnameOverride: "quickstart"

# Version of Elasticsearch.
#
version: 8.13.3 # elasticsearch 版本

# Elasticsearch Docker image to deploy
#
# image:

# Labels that will be applied to Elasticsearch.
#
labels: {}

# Annotations that will be applied to Elasticsearch.
#
annotations: {}

# Settings for configuring Elasticsearch users and roles.
# ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-users-and-roles.html
#
auth: {}

# Settings for configuring stack monitoring.
# ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-stack-monitoring.html
#
monitoring: {}
  # metrics:
  #   elasticsearchRefs:
  #   - name: monitoring
  #     namespace: observability
  # logs:
  #   elasticsearchRefs:
  #   - name: monitoring
  #     namespace: observability

# Control the Elasticsearch transport module used for internal communication between nodes.
# ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-transport-settings.html
#
transport: {} # 控制集群各节点的连接方式
  # service:
  #   metadata:
  #     labels:
  #       my-custom: label
  #   spec:
  #     type: LoadBalancer
  # tls:
  #   subjectAltNames:
  #     - ip: 1.2.3.4
  #     - dns: hulk.example.com
  #   certificate:
  #     secretName: custom-ca

# Settings to control how Elasticsearch will be accessed.
# ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-accessing-elastic-services.html
#
http:  # 资源对象service的设置
  service:
    metadata:
      labels:
        inboc-es-service: label
    spec:
      type: NodePort
  # tls:
  #   selfSignedCertificate:
  #     # To fully disable TLS for the HTTP layer of Elasticsearch, simply
  #     # set the below field to 'true', removing all other fields.
  #     disabled: false
  #     subjectAltNames:
  #       - ip: 1.2.3.4
  #       - dns: hulk.example.com
  #   certificate:
  #     secretName: custom-ca

# Control Elasticsearch Secure Settings.
# ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-es-secure-settings.html#k8s-es-secure-settings
#
secureSettings: # 使用 elasticsearch 密钥库
  - secretName: snapshot-settings # minio_key
  # Projection of secret keys to specific paths
  # - secretName: gcs-secure-settings
  #   entries:
  #   - key: gcs.client.default.credentials_file
  #   - key: gcs_client_1
  #     path: gcs.client.client_1.credentials_file
  #   - key: gcs_client_2
  #     path: gcs.client.client_2.credentials_file

# Settings for limiting the number of simultaneous changes to an Elasticsearch resource.
# ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-update-strategy.html
#
updateStrategy: # 更新策略
   changeBudget:
     maxSurge: 3
     maxUnavailable: 1

# Controlling of connectivity between remote clusters within the same kubernetes cluster.
# ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-remote-clusters.html
#
remoteClusters: {}
  # - name: cluster-two
  #   elasticsearchRef:
  #     name: cluster-two
  #     namespace: ns-two

# VolumeClaimDeletePolicy sets the policy for handling deletion of PersistentVolumeClaims for all NodeSets.
# Possible values are DeleteOnScaledownOnly and DeleteOnScaledownAndClusterDeletion.
# By default, if not set or empty, the operator sets DeleteOnScaledownAndClusterDeletion.
#
volumeClaimDeletePolicy: "DeleteOnScaledownOnly" # 控制当删除es节点时，PVC是否删除

# Settings to limit the disruption when pods need to be rescheduled for some reason such as upgrades or routine maintenance.
# By default, if not set, the operator sets a budget that doesn't allow any pod to be removed in case the cluster is not green or if there is only one node of type `data` or `master`.
# In all other cases the default PodDisruptionBudget sets `minUnavailable` equal to the total number of nodes minus 1.
# To completely disable the pod disruption budget set `disabled` to true.
#
podDisruptionBudget: # 限制自愿中断时，应用最小可用性能
 spec:
   minAvailable: 2
   selector:
     matchLabels:
       elasticsearch.k8s.elastic.co/cluster-name: 
 disabled: true

# Used to check access from the current resource to a resource (for ex. a remote Elasticsearch cluster) in a different namespace.
# Can only be used if ECK is enforcing RBAC on references.
#
# serviceAccountName: ""

# Number of revisions to retain to allow rollback in the underlying StatefulSets.
# By default, if not set, Kubernetes sets 10.
#
# revisionHistoryLimit: 2

# Node configuration settings.
# The node roles which can be configured here are:
# - "master"
# - "data_hot"
# - "data_cold"
# - "data_frozen"
# - "data_content"
# - "ml"
# - "ingest"
# ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-node-configuration.html
#
nodeSets: # 节点控制器
- name: master # 定义节点的名称，注意如果需要定义节点的角色，需要在配置文件中定义，与该名称无关
  count: 4 # 控制副本数量
  config: # 该es节点集群的配置文件
    # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
    # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
    # and leave node.store.allow_mmap unset.
    # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
    #
    node.store.allow_mmap: false
    s3.client.default.endpoint: https://minio.inboc.top # 连接到Minio客户端的URL
    s3.client.default.protocol: https # 连接方式
  podTemplate:
    # The following spec is exactly the Kubernetes Core V1 PodTemplateSpec. Any fields within the PodTemplateSpec
    # are supported within the 'spec' field below.  Please see below documentation for the exhaustive list of fields.
    #
    # https://v1-24.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#podtemplatespec-v1-core
    #
    # Only the commonly overridden/used fields will be noted below.
    #
    spec:

    # If specified, the pod's scheduling constraints
    # https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html
    # https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    # affinity:
    #   nodeAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       nodeSelectorTerms:
    #       - matchExpressions:
    #         - key: topology.kubernetes.io/zone
    #           operator: In
    #           values:
    #           - antarctica-east1
    #           - antarctica-west1

#      affinity: # 亲和性设置，注意注释的部分是默认设置，注释也会执行，可以被覆盖
#        podAntiAffinity:
#          preferredDuringSchedulingIgnoredDuringExecution:
#            - weight: 100
#              podAffinityTerm:
#                labelSelector:
#                  matchLabels:
#                    elasticsearch.k8s.elastic.co/cluster-name: test-es
#                topologyKey: kubernetes.io/hostname

    # Containers array.  Should only be used to customize the 'elasticsearch' container using the following fields.
      containers:
      - name: elasticsearch

        # List of environment variables to set in the 'elasticsearch' container.
        # https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/
        # env:
        env:
        - name: ES_JAVA_OPTS
          value: "-Djavax.net.ssl.trustStore=/usr/share/elasticsearch/config/custom-truststore/cacerts -Djavax.net.ssl.keyStorePassword=changeit"
 #       - name: ES_JAVA_OPTS # Java虚拟机堆内存
 #         value: -Xms2g -Xmx2g
        # Compute Resources required by this container.
        resources:
          # Requests describes the minimum amount of compute resources required. If Requests is omitted for a container,
          # it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value.
          #
          # Defaults used by the ECK Operator, if not specified, are below
          limits:
            cpu: 1
            memory: 2Gi
          requests:
            cpu: 1
            memory: 2Gi


          # Example increasing both the requests and limits values:
          # limits:
          #   cpu: 4
          #   memory: 8Gi
          # requests:
          #   cpu: 1
          #   memory: 8Gi

#        volumeMounts:
#          - name: elasticsearch-logs
#            mountPath: /usr/share/elasticsearch/logs

        # SecurityContext defines the security options the container should be run with.
        # If set, the fields of SecurityContext override the equivalent fields of PodSecurityContext.
        #
        # These typically are set automatically by the ECK Operator, and should only be adjusted
        # with the full knowledge of the effects of each field.
        #
        securityContext: # 定义容器安全管理

          # Whether this container has a read-only root filesystem. Default is false.
            readOnlyRootFilesystem: false

          # The GID to run the entrypoint of the container process. Uses runtime default if unset.
            runAsGroup: 1000

          # Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure
          # that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed.
            runAsNonRoot: true

          # The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified.
            runAsUser: 1000

    # ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling any of the images used by this PodSpec.
    # https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod
#     imagePullSecrets:
#     - name: "image-pull-secret"

    # List of initialization containers belonging to the pod.
    #
    # Common initContainers include setting sysctl, or in 7.x versions of Elasticsearch,
    # installing Elasticsearch plugins.
    #
    # https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
      initContainers: # 初始化容器
       - command:
         - sh
         - "-c"
         - sysctl -w vm.max_map_count=262144
         name: sysctl
         securityContext:
           privileged: true
#       - command:
#         - sh
#         - "-c" 
#         - bin/elasticsearch-plugin remove --purge analysis-icu ; bin/elasticsearch-plugin install --batch analysis-icu
#         name: install-plugins   # 安装需要的插件
#         securityContext:
#           privileged: true


    # NodeSelector is a selector which must be true for the pod to fit on a node. Selector which must match a node's labels for the pod to be scheduled on that node.
    # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    # https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html
    # nodeSelector:
    #   diskType: ssd
    #   environment: production

    # If specified, indicates the pod's priority. "system-node-critical" and "system-cluster-critical" are two special keywords which indicate the highest priorities with the former being the highest priority.
    # Any other name must be defined by creating a PriorityClass object with that name. If not specified, the pod priority will be default or zero if there is no default.
    # https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/
    # priorityClassName: ""

    # SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field.
    # See previously defined 'securityContext' within 'podTemplate' for all available fields.
    # securityContext: {}

    # ServiceAccountName is the name of the ServiceAccount to use to run this pod.
    # https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    # serviceAccountName: ""

    # Optional duration in seconds to wait for the Elasticsearch pod to terminate gracefully.
    # terminationGracePeriodSeconds: 30s

    # If specified, the pod's tolerations that will apply to all containers within the pod.
    # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    # tolerations:
    # - key: "node-role.kubernetes.io/elasticsearch"
    #   effect: "NoSchedule"
    #   operator: "Exists"

    # TopologySpreadConstraints describes how a group of pods ought to spread across topology domains.
    # Scheduler will schedule pods in a way which abides by the constraints. All topologySpreadConstraints are ANDed.
    #
    # These settings are generally applied within each `nodeSets[].podTemplate` field to apply to a specific Elasticsearch nodeset.
    #
    # https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html
    # topologySpreadConstraints: {}

    # List of volumes that can be mounted by containers belonging to the pod.
    # https://kubernetes.io/docs/concepts/storage/volumes
    # volumes: []
    volumeClaimTemplates: # 默认容器内挂载路径/usr/share/elasticsearch/data
      - metadata:
          name: elasticsearch-data # 名字是固定的，不能修改
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 2Gi
          storageClassName: longhorn

#      - metadata:
#          name: elasticsearch-logs
#        spec:
#          accessModes:
#            - ReadWriteOnce
#          resources:
#            requests:
#              storage: 1Gi
#          storageClassName: longhorn
```


\

# 3. Xpack+kibana 监控+搜集ES集群数据功能

X-Pack 是一个扩展包，它为 Elasticsearch 和 Kibana 提供了一系列增强功能，包括安全性、监控、报告、机器学习等，在Elasticsearch 8.0 后自动安装并且开启。

官方文档说明：

<https://www.elastic.co/guide/en/elasticsearch/reference/8.13/monitor-elasticsearch-cluster.html>

```yaml
# nodeSets.config
 nodeSets:
  - config:
      xpack.monitoring.collection.enabled: true
      
  # 当 xpack.monitoring.collection.enabled 设置为 true 时
  # Elasticsearch 和 Kibana 将开始收集监控数据，并将这些数据发送到一个指定的监控集群或存储在本地集群中。
  # 这些数据包括但不限于集群的健康状况、性能指标、节点信息以及各种索引的统计数据。
```

Elasticseasrch默认会将收集的数据自建以日期命名的索引保存在本地集群中


 ![](attachments/49aabb6b-fad3-45bb-8514-ce5ced845b53.png)


\
 ![](attachments/0b3aa884-e3ce-4afb-a61a-1ee9c81c10ca.png)


在Kibana页面中开启收集数据

#### **Stack Monitoring**


 ![](attachments/1bce5bab-6bd1-4087-b338-491621e3fdec.png)


集群监控


 ![](attachments/d5a60b51-b0c8-4215-af4d-e7e0b47c4b0a.png)


节点监控


 ![](attachments/3391c4b6-cb47-4902-9e7c-b2d7f8f5f021.png)


\
 ![](attachments/b7271452-1af2-481e-ba3f-400606784514.png)


索引数据监控


 ![](attachments/f2e0e10d-da0f-4e0c-831f-18c0a1d18db4.png)


\
#### **Stach Management**


1. **索引管理（Index Management）**

用户可以查看集群中所有索引的列表，包括每个索引的健康状态、文档数量和存储空间使用情况。此外，还可以执行一些操作，如创建和删除索引、管理索引设置和映射、以及执行索引的开启和关闭操作。


2. **索引生命周期管理（Index Lifecycle Policies）**

在这里，用户可以创建和管理索引生命周期策略，这些策略定义了索引从创建到删除的整个生命周期。策略包括何时滚动索引、何时将索引移动到不同的存储介质（如从热存储到冷存储），以及何时删除旧索引。


3. **快照和恢复（Snapshot and Restore）**


 ![](attachments/5dec4698-f256-454c-be81-97d23e0bdf6d.png)


快照策略


 ![](attachments/28ecc302-b5b2-40a3-afe6-4695b4f316b0.png)



4. **安全（Security）**

这部分允许用户配置和管理安全相关的设置，包括用户认证和授权。可以在这里创建和管理用户、角色、API 密钥和角色映射。此外，还可以配置安全特性，如启用 TLS/SSL 加密，以确保数据传输的安全。


 ![](attachments/9198b409-6a7a-4231-83ab-67d975e02555.png)


# 4.有关节点维护，集群级分片分配设置

官方文档：

<https://www.elastic.co/guide/en/elasticsearch/reference/8.13/modules-cluster.html>

#### 集群级分片分配设置

* `cluster.routing.allocation.enable`

（动态）启用或禁用特定类型分片的分配：


1.  all-（默认）允许所有类型的分片进行分片分配。
2.  primaries- 仅允许为主分片分配分片。 
3.  new_primaries- 只允许为新索引的主分片分配分片。 
4.  none- 任何索引都不允许进行任何类型的分片分配。


**暂停分片分配**

```
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": "none"
  }
}
```

新创建索引，分片不会再分配到节点上


 ![](attachments/d6cd5b16-0f9e-4138-b39a-3cd495b26d31.png) ![](/home/chonghao.zhang/nextcloud/inboc-sys-junior/outline/attachments/image-20240513100659708.png)


\
## **节点维护**

节点如果需要维护，控制分片不要分在需要维护的某个节点上

节点如果需要维护，控制分片不要分在某个节点上

\#**方案一**

```
PUT _cluster/settings
{
  "persistent" : {
    "cluster.routing.allocation.exclude._name" : "elasticsearch-es-master-0"
  }
}
# 不要将分片分发到节点"elasticsearch-es-master-0"
```

**elasticsearch-es-master-0节点的分片会自动全部分发到其他节点上，需要注意其他节点的负载情况**


 ![](attachments/fc2a2013-f2c2-4065-ba6a-1b1caa6c981b.png) ![](/home/chonghao.zhang/nextcloud/inboc-sys-junior/outline/attachments/image-20240513094141531.png)


集群未分配索引前状态


 ![](attachments/38de4811-090d-482d-9b1e-451c1ac61f59.png) ![](/home/chonghao.zhang/nextcloud/inboc-sys-junior/outline/attachments/image-20240513102719617.png)


集群分片重新分配状态


 ![](attachments/4f2f2661-0dc5-4ce4-a9e2-522c52d92f2b.png) ![](/home/chonghao.zhang/nextcloud/inboc-sys-junior/outline/attachments/image-20240513102447005.png)


**因为没有修改集群分片分发策略，所以不影响新的索引创建**


\#**方案二**

关闭自动分发分片策略

```
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": "none"
  }
}
```

删除master-2节点，删除PV


 ![](attachments/7e3ed8f7-2f93-4621-8f4c-81ce255284ec.png) ![](/home/chonghao.zhang/nextcloud/inboc-sys-junior/outline/attachments/image-20240513104453883.png)


不会将master-2节点数据写入集群其他节点


 ![](attachments/a1656ec2-d586-4f93-9fac-cc289d5ddeb8.png)


写入索引文档数据测试

```
//停机mater-2节点写入数据测试
PUT /tdrtrt_esataestst/_doc/1
{
  "field1": "value1",
  "field2": "value2",
  "field3": "value3"
}
```

**数据写入正常，但是由于关闭了分片自动分发策略，所以此时无法写入新的索引规则**

 ![](/home/chonghao.zhang/nextcloud/inboc-sys-junior/outline/attachments/image-20240513104540328.png)


```
# 查询索引中的文档数据
GET /tdrtrt_esataestst/_search
{
  "query": {
    "match_all": {}
  }
}
```

重新创建节点


 ![](attachments/10f3559b-de70-4caf-a6e1-abdc774df79d.png)


\
 ![](attachments/3b3e9ced-7315-4e4e-897f-8a887afca840.png) ![](/home/chonghao.zhang/nextcloud/inboc-sys-junior/outline/attachments/image-20240513104624260.png) ![](/home/chonghao.zhang/nextcloud/inboc-sys-junior/outline/attachments/image-20240513104817126.png)


打开自动分发分片设置

```
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": "all"
  }
}
# 默认es会重新分发分片
```

数据恢复成功


 ![](attachments/5860f321-1586-43af-8b6c-ba5e7e69afdc.png) ![](/home/chonghao.zhang/nextcloud/inboc-sys-junior/outline/attachments/image-20240513105019701.png)


\
## **关于有分片未被分配的原因诊断**

```yaml
使用 _cluster/reroute API 附带 explain 参数来获取关于为什么分片无法被正常分配的更多信息：

POST _cluster/reroute?explain=true
# 查询所有分片的信息

GET /_cluster/allocation/explain?pretty
# 查询节点无法接收分片的原因
# 如果正常es集群不存在未分配的切片，返回错误码400,只有集群中存在未分配的分片才会返回原因信息
```


#